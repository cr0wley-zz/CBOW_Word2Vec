#Continuous-bag of words (CBOW)

The Continuous-bag of words (CBOW), along with Skip-gram model, is used frequently in NLP using deep learning.
Given a range N of context words before and after the target word, it tries to predict the current(target) word.

This code is an implementation of an exercise given by PyTorch tutorials in "Getting Dense Word Embeddings" of Word Embeddings of the following link:

	https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#getting-dense-word-embeddings


##References

For further studies about word embeddings, read the papers below:

1. Efficient Estimation of Word Representations in Vector Space
2. word2vec Explained: Deriving Mikolov et al's Negative-Sampling Word-Embedding Method
3. Distributed Representations of Words and Phrases and their Compositionality



 
